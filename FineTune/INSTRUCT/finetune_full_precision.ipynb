{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib\n",
    "!pip install trl\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "# download dataset\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dataset_dir = \"./Data_Finetune_Mistral.jsonl\"\n",
    "dataset = load_dataset('json', data_files=dataset_dir, split='train')\n",
    "\n",
    "def format_instruction(sample):\n",
    "    return f\"\"\"You are a personal assistant . Help users to learn  fundamentals and techniques. Recommend ideal  exercises to help users to improve their  skills.\n",
    "        ### Instruction:{sample[\"instruction\"]} ### Response:\"\"\"\n",
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model id to fine-tune\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# load model full precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_cache=False, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# load tokenizer, pad short samples with end of sentence token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "new_model = \"mistral-7b-golf-assistant_full_precision\"\n",
    "model_args = TrainingArguments(\n",
    "    output_dir=\"mistral-7b-golf-assistant_full_precision\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    bf16=True, # full precision = 32 bit, mixed precision = 16 bit\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "# Supervised Fine-Tuning Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction,\n",
    "    args=model_args,\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "#trainer.save_model()\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.model.save_pretrained(\"mistral-7b-golf-assistant_full_precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "base_model = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model, \n",
    "    use_cache=False, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model_reload, \"mistral-7b-golf-assistant_full_precision\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "adapters_name = \"mistral-7b-golf-assistant_full_precision\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapters_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def format_instruction(sample):\n",
    "    return f\"\"\"You are a personal assistant . Help users to learn  fundamentals and techniques. Recommend ideal  exercises to help users to improve their  skills.\n",
    "        ### Instruction:{sample} ### Response:\"\"\"\n",
    "\n",
    "\n",
    "def call_inference(sample):\n",
    "\n",
    "    #sample = dataset[randrange(len(dataset))]\n",
    "    prompt = format_instruction(sample)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **input_ids, \n",
    "            max_new_tokens=200, \n",
    "            do_sample=True, \n",
    "            top_p=0.9,\n",
    "            temperature=0.6\n",
    "        )\n",
    "\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(\"full output :\", outputs)\n",
    "    output = outputs[0][len(prompt):]\n",
    "    #print(\"output :\", output)\n",
    "\n",
    "    if output:\n",
    "        if \"### Instruction:\" in output:\n",
    "            output = output.split(\"### Instruction:\")[0].strip()\n",
    "\n",
    "        instruction = sample\n",
    "        output = output\n",
    "\n",
    "        print(f\"Instruction: \\n{instruction}\\n\\n\")\n",
    "        print(f\"Generated output: \\n{output}\\n\\n\\n\")   \n",
    "    else:\n",
    "        print(\"I am sorry, I did not understand that. Could you please rephrase your question?\")\n",
    "    #return instruction, output, groud_truth\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"User: \")\n",
    "    call_inference(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
