{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//miniconda3/envs/torch2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-08 18:34:04.350448: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-08 18:34:04.350479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-08 18:34:04.351115: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-08 18:34:04.355690: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-08 18:34:04.976433: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
    "#!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "#model_name= \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False #(double quantization)\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "standalone_query_generation_pipeline = pipeline(\n",
    "    model=mistral_model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "standalone_query_generation_llm = HuggingFacePipeline(pipeline=standalone_query_generation_pipeline)\n",
    "\n",
    "response_generation_pipeline = pipeline(\n",
    "    model=mistral_model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.6,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "response_generation_llm = HuggingFacePipeline(pipeline=response_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!playwright install \n",
    "#!playwright install-deps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Articles to index\n",
    "articles = [\"https://www..com/blog/the-need-for-speed/\",\n",
    "            \"https://www..com/blog/improve-your-green-reading-with-these-easy-steps/\",\n",
    "            \"https://www..com/blog/make-the-most-out-of-your--practice/\",\n",
    "            \"https://www..com/products/indoor/features/\",\n",
    "            \"https://www..com/blog/strokes-gained-/\",\n",
    "            \"https://www..com/blog/get-in-the-hole/\",\n",
    "            \"https://www..com/blog/visual-cues/\",\n",
    "            \"https://www..com/blog/your-trackman-for-/\",\n",
    "            \"https://www..com/blog/putter-fitting-with-martin-stecher/\"]\n",
    "\n",
    "loader = AsyncChromiumLoader(articles)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 704, which is longer than the specified 600\n",
      "Created a chunk of size 671, which is longer than the specified 600\n",
      "Created a chunk of size 751, which is longer than the specified 600\n",
      "Created a chunk of size 725, which is longer than the specified 600\n",
      "Created a chunk of size 827, which is longer than the specified 600\n",
      "Created a chunk of size 820, which is longer than the specified 600\n",
      "Created a chunk of size 778, which is longer than the specified 600\n",
      "Created a chunk of size 704, which is longer than the specified 600\n",
      "Created a chunk of size 917, which is longer than the specified 600\n",
      "Created a chunk of size 763, which is longer than the specified 600\n",
      "Created a chunk of size 946, which is longer than the specified 600\n",
      "Created a chunk of size 676, which is longer than the specified 600\n",
      "Created a chunk of size 1006, which is longer than the specified 600\n",
      "Created a chunk of size 749, which is longer than the specified 600\n",
      "Created a chunk of size 636, which is longer than the specified 600\n",
      "Created a chunk of size 751, which is longer than the specified 600\n",
      "Created a chunk of size 767, which is longer than the specified 600\n",
      "Created a chunk of size 673, which is longer than the specified 600\n",
      "Created a chunk of size 662, which is longer than the specified 600\n",
      "Created a chunk of size 767, which is longer than the specified 600\n",
      "Created a chunk of size 1416, which is longer than the specified 600\n",
      "Created a chunk of size 1210, which is longer than the specified 600\n",
      "Created a chunk of size 1173, which is longer than the specified 600\n",
      "Created a chunk of size 640, which is longer than the specified 600\n",
      "Created a chunk of size 975, which is longer than the specified 600\n",
      "Created a chunk of size 1421, which is longer than the specified 600\n",
      "Created a chunk of size 1062, which is longer than the specified 600\n",
      "Created a chunk of size 868, which is longer than the specified 600\n",
      "Created a chunk of size 704, which is longer than the specified 600\n",
      "Created a chunk of size 840, which is longer than the specified 600\n",
      "Created a chunk of size 771, which is longer than the specified 600\n",
      "Created a chunk of size 721, which is longer than the specified 600\n",
      "Created a chunk of size 730, which is longer than the specified 600\n",
      "Created a chunk of size 1018, which is longer than the specified 600\n",
      "Created a chunk of size 658, which is longer than the specified 600\n",
      "Created a chunk of size 608, which is longer than the specified 600\n",
      "Created a chunk of size 726, which is longer than the specified 600\n",
      "Created a chunk of size 719, which is longer than the specified 600\n",
      "Created a chunk of size 763, which is longer than the specified 600\n",
      "Created a chunk of size 637, which is longer than the specified 600\n",
      "Created a chunk of size 1207, which is longer than the specified 600\n",
      "Created a chunk of size 1532, which is longer than the specified 600\n",
      "Created a chunk of size 627, which is longer than the specified 600\n",
      "Created a chunk of size 680, which is longer than the specified 600\n",
      "Created a chunk of size 1000, which is longer than the specified 600\n",
      "Created a chunk of size 675, which is longer than the specified 600\n",
      "Created a chunk of size 787, which is longer than the specified 600\n",
      "Created a chunk of size 854, which is longer than the specified 600\n",
      "Created a chunk of size 787, which is longer than the specified 600\n",
      "Created a chunk of size 601, which is longer than the specified 600\n",
      "Created a chunk of size 633, which is longer than the specified 600\n",
      "Created a chunk of size 623, which is longer than the specified 600\n",
      "Created a chunk of size 704, which is longer than the specified 600\n",
      "Created a chunk of size 1054, which is longer than the specified 600\n",
      "Created a chunk of size 1180, which is longer than the specified 600\n",
      "Created a chunk of size 601, which is longer than the specified 600\n",
      "Created a chunk of size 771, which is longer than the specified 600\n",
      "Created a chunk of size 642, which is longer than the specified 600\n",
      "Created a chunk of size 1132, which is longer than the specified 600\n",
      "Created a chunk of size 790, which is longer than the specified 600\n",
      "Created a chunk of size 1335, which is longer than the specified 600\n",
      "Created a chunk of size 704, which is longer than the specified 600\n",
      "Created a chunk of size 1680, which is longer than the specified 600\n",
      "Created a chunk of size 1185, which is longer than the specified 600\n",
      "Created a chunk of size 898, which is longer than the specified 600\n",
      "Created a chunk of size 1297, which is longer than the specified 600\n",
      "Created a chunk of size 1102, which is longer than the specified 600\n",
      "Created a chunk of size 705, which is longer than the specified 600\n",
      "Created a chunk of size 901, which is longer than the specified 600\n",
      "Created a chunk of size 855, which is longer than the specified 600\n",
      "Created a chunk of size 978, which is longer than the specified 600\n",
      "Created a chunk of size 1333, which is longer than the specified 600\n",
      "Created a chunk of size 1064, which is longer than the specified 600\n",
      "Created a chunk of size 778, which is longer than the specified 600\n"
     ]
    }
   ],
   "source": [
    "# Converts HTML to plain text \n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=0) # @TODO: Make this configurable and minimize the chunk size\n",
    "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(chunked_documents, HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
    "\n",
    "retriever = db.as_retriever(k = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGCHAIN Prompt Template - CONVERSATIONAL LLM\n",
    "\n",
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    _template = \"\"\"\n",
    "    [INST] \n",
    "    Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language, that can be used to query a FAISS index. This query will be used to retrieve documents with additional context. \n",
    "\n",
    "    Let me share a couple examples that will be important. \n",
    "\n",
    "    If you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\n",
    "\n",
    "    ```\n",
    "    Chat History:\n",
    "\n",
    "    Follow Up Input: what are fundamentals of golf?\n",
    "    Standalone Question:\n",
    "    what are fundamentals of golf?\n",
    "    ```\n",
    "\n",
    "    If this is the second question onwards, you should properly rephrase the question like this:\n",
    "\n",
    "    ```\n",
    "    Chat History:\n",
    "    what are fundamentals of golf?\n",
    "    AI: \n",
    "    Fundamentals of golf are speed, line, and green reading.\n",
    "\n",
    "    Follow Up Input: Can you explain them?\n",
    "    Standalone Question:\n",
    "    Can you explain the fundamentals of golf?\n",
    "    ```\n",
    "\n",
    "    Now, with those examples, here is the actual chat history and input question.\n",
    "\n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "\n",
    "    Follow Up Input: {question}\n",
    "    Standalone question:\n",
    "    [your response here]\n",
    "    [/INST] \n",
    "    \"\"\"\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "[INST] \n",
    "You are an AI assistant that helps users to improve their  skills. Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "[/INST] \n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    " return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "\n",
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | standalone_query_generation_llm,\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | response_generation_llm,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"context\": final_inputs[\"context\"]\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def call_conversational_rag(question, chain, memory):\n",
    "    \"\"\"\n",
    "    Calls a conversational RAG (Retrieval-Augmented Generation) model to generate an answer to a given question.\n",
    "\n",
    "    This function sends a question to the RAG model, retrieves the answer, and stores the question-answer pair in memory \n",
    "    for context in future interactions.\n",
    "\n",
    "    Parameters:\n",
    "    question (str): The question to be answered by the RAG model.\n",
    "    chain (LangChain object): An instance of LangChain which encapsulates the RAG model and its functionality.\n",
    "    memory (Memory object): An object used for storing the context of the conversation.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the generated answer from the RAG model.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # Prepare the input for the RAG model\n",
    "    inputs = {\"question\": question}\n",
    "\n",
    "    # Invoke the RAG model to get an answer\n",
    "    result = chain.invoke(inputs)\n",
    "    \n",
    "    # Save the current question and its answer to memory for future context\n",
    "    memory.save_context(inputs, {\"answer\": result[\"answer\"]})\n",
    "    end = time.time()\n",
    "    print(\"response time :\",end - start)\n",
    "    # Return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//miniconda3/envs/torch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//miniconda3/envs/torch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response time : 12.468083381652832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'To improve your  skills in golf, consider focusing on the following areas:\\n1. Alignment: Ensure your body, club, and putter face are all aligned correctly towards your target.\\n2. Tempo: Maintain a consistent tempo throughout your stroke for better control and accuracy.\\n3. Contact: Make solid contact with the ball by keeping your putter face square at impact.\\n4. Green Reading: Understand the contours and slopes of the green to make informed decisions about the speed and direction of your putts.\\n5. Practice: Regularly practice  drills and techniques to build muscle memory and confidence.\\n\\nAdditionally, you can use reference points to prepare yourself for various options on the green. Find a specific slope percentage and practice hitting putts to that target regularly. This will help you replicate those putts on the course and improve your confidence in your green reading abilities. Another effective method is to map out your putt using',\n",
       " 'question': \"To help you improve your  in golf, I'd suggest focusing on the following areas:\\n1. Alignment: Ensure your body, club, and putter face are all aligned correctly towards your target.\\n2. Tempo: Maintain a consistent tempo throughout your stroke for better control and accuracy.\\n3. Contact: Make solid contact with the ball by keeping your putter face square at impact.\\n4. Green Reading: Understand the contours and slopes of the green to make informed decisions about the speed and direction of your putts.\\n5. Practice: Regularly practice  drills and techniques to build muscle memory and confidence.\\n\\nStandalone question:\\nWhat are effective ways to improve my  skills in golf?\",\n",
       " 'context': '3\\\\. Finding some reference points for yourself can help you tremendously on\\nthe green. Just because you never face the same putt on the green twice,\\ndoesn’t mean that you can’t prepare yourself for various options. On average,\\nyou are facing slopes of around 2-3% and it helps to know where you need to\\naim at on those slopes. Therefore, the next time you practice, find yourself a\\nspecific slope percentage and write down how many balls, cups, or whatever\\nother measurement you choose, you need to aim at. If you keep a list like this\\nand additionally practice hitting those putts regularly, you will find it\\neasier on the course to replicate those putts. This will give you confidence\\nin your green reading abilities and ultimately improve your \\nperformance.\\n\\n1. Going back to the geometry of the putt, a great way to improve your green reading is to map out your putt. Ideally, you use coins to place on the green. Place them at the apex, the hole entry point as well as the aiming point. Check again if you placed them correctly and then proceed to putt the putt. Pay close attention to the path your ball travels and if you managed to start the ball on your intended line. In case you realize that the coins you placed are not in the right spot, place a second set of coins on the points you believe are correct. This way you will be able to see how far off you were with your initial guess.\\n\\nIf you don’t own a  System and don’t know the stimpmeter of your\\n mat, you can ask your course superintendent about the green speed on\\nthe practice green for that day. Now you can choose a straight putt and start\\nhitting balls from there. While doing this make sure you know the distance to\\nthe hole and really focus on your  stroke and how far back you take the\\nputter.\\n\\nObviously, the study that was conducted is not representative, but it does\\nshow nicely, how important it is to know your personal preferences and cross-\\nreference them with real results. First of all, it does matter if someone\\ndescribed a bird to you and this is the only way you can build a mental\\npicture of a bird, or if you have seen various birds yourself and have a\\nmental picture based on that. As mentioned above, most people have never seen\\na putt and base their predictions on someone else’s perception. Therefore, if\\nyou have never seen a putt before, and don’t own a  System, try to map\\nout the line the ball will travel on and also the point you need to aim at.\\nThen ask yourself, which version feeds into your mental picture. Additionally,\\nyou can hit a couple of putts and try to feel which version feels more natural\\nfor you. Next, you can try to run different visuals in your mind. Use your own\\ncreativity while determining if you feel more comfortable by visualizing a\\npoint, a line, or a rolling ball. And lastly, cross-reference those ideas with\\nresults. That way, you can determine in which direction you should shift your\\nfocus during future  practice. By following these steps, you will\\nseverely increase the efficiency of your  practice, as well as build up\\nconfidence and hole more putts.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"how can i improve my ?\"\n",
    "call_conversational_rag(question, final_chain, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//miniconda3/envs/torch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//miniconda3/envs/torch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response time : 13.167700290679932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"For someone new to  in golf, here are some recommendations to improve your skills:\\n\\n1. Proper alignment: Learn how to align your body, club, and putter face correctly towards your target. You can use reference points such as a line on the ground or a specific slope percentage to help with alignment.\\n2. Consistent tempo: Develop a consistent tempo throughout your stroke for better control and accuracy. You can practice this by focusing on the length and rhythm of your swing.\\n3. Solid contact: Practice making solid contact with the ball by keeping your putter face square at impact. You can use drills such as hitting balls to specific targets to help improve your contact.\\n4. Basic green reading: Start learning the basics of interpreting contours and slopes on the green to make informed decisions about the speed and direction of your putts. You can practice this by observing the putt's path and adjusting your aim accordingly.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what can you recommend me to improve my  skills? i have no idea about .\"\n",
    "text = call_conversational_rag(question, final_chain, memory)\n",
    "text['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//miniconda3/envs/torch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home//miniconda3/envs/torch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response time : 14.089851379394531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the context provided, here's how I would answer the question:\\n\\nTo create a personalized training plan for improving your  skills, consider the following steps:\\n\\n1. Assess your current abilities: Evaluate your strengths and weaknesses in , including aim, distance control, green reading, and mental focus. You can do this by observing your  performance on the course or during practice sessions.\\n2. Set specific, measurable goals: Establish clear objectives for your  improvement. For example, you might aim to reduce the number of three-putts you have per round, improve your average number of putts per round, or achieve a certain  percentage. Make sure your goals are specific, measurable, achievable, relevant, and time-bound.\\n3. Find reference points: Identify specific slope percentages on the practice green and work on aiming accurately at those targets. This will help you build confidence in your green\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "question = \"can you help me to get better? Can you prepare a training plan for me\"\n",
    "text = call_conversational_rag(question, final_chain, memory)\n",
    "text['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//miniconda3/envs/torch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//miniconda3/envs/torch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response time : 10.564454078674316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To answer your question, yes, ladder drills can be effective for improving speed control in golf . By setting up a series of putts with increasing lengths, you can focus on maintaining consistent stroke length and tempo throughout the drill. This will help you train yourself to control your speed more effectively and develop a better feel for the distance required for various putts. Ultimately, this can lead to improved performance on the green.\\n\\nHowever, it's important to note that ladder drills may not completely eliminate the need for guessing and trial and error scenarios, especially when it comes to identifying whether an error was due to initial ball direction or speed control. To address this issue, using  tools such as a  System can provide valuable feedback and help improve your accuracy and consistency in both areas.\\n\\nAdditionally, practicing your speed control on the same green and under the same conditions can help ensure more accurate results and make it easier to replicate your findings. Remember\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"i heard that ladder drills are good for speed control. what do you think ? \"\n",
    "text = call_conversational_rag(question, final_chain, memory)\n",
    "text['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home//miniconda3/envs/torch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home//miniconda3/envs/torch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response time : 6.744657039642334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To answer your question using the context provided, we cannot directly provide information about specific drills for improving speed control in golf  from the text. However, we can suggest looking into the \"Features\" section of the \"Indoor\" and \"Outdoor\" products related to  practice, such as the P7, P7 Plus, P8, P10, P12, C1, CX1, C2, CX2, and Moving Green, as they may offer features or tools designed to help improve speed control in golf . Additionally, checking out the \"Blog\" section for articles on strokes gained  and other  techniques and drills could also be beneficial.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is the best exercise for speed control \"\n",
    "text = call_conversational_rag(question, final_chain, memory)\n",
    "text['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
